With_sentinels_long-prompt_weighted-loss: First runs, sentinels added to use as marker for key-tokens that will receive higher loss-weights. Option-A: only sentinels and first token get higher loss. Option-B: sentinels and entire key-phrase get higher loss.

+) V2_batch6-seedxx 1st run: Run with active early stopping before full epoch, stopping criteria: avg_eval_loss. Note: starting from these runs, the sentinel tokens themselves also get increased weights. There was another run before this run that uses accuracy on key-tokens as eval metric but it was very overfitted, this run had no increased loss on sentinel, only key-tokens after. ("keyword_weight_phases": [1.25, 1.5, 1.5])

+) V2_batch6-seedxx 2nd run: Runs for full epoch, checkpoints at step 2199 and 2299 manually saved to see performance at about 85%-90% epoch. Reason was from some runs it was observed that eval loss starts to increase strongly at around this point. Not a hard rule, just one instance that was observed.

+) V2_batch6-seed42 1st run "first-eval_chkpoint": same as V2_batch6-seedxx 1st run described above, but saved at the first time the evaluate_model method is called. (step 199)

+) V3_2xW_bxx-seedxx: Only for Option A. runs from V2_batch6-seedxx but with key-tokens weights increased. ("keyword_weight_phases": [2, 3, 3])

+) V2_batch12: First Batch12 runs only for seed 42 and did not run full epoch (Both OpA and OpB). Then for seed 23 and seed 322, both OpA and OpB were run for at least one epoch. But for Option B, unfortunately the code missed logic to save checkpoint when first epoch is finished and thus does not have a full epoch checkpoint.

------------------------------------------------------

Without_sentinels_long-prompt_equal-loss: Removed all Sentinels, user/assistant-prompt remains the same, thus all tokens now have equal loss. Main idea is to test if Sentinels and increased weights actually increase performance.

+) b6_1st-run: First run with same prompt, one epoch, on the full training dataset.

+) b6_rerun: Same as before but this time a checkpoint at step 2299 was saved for comparison with previous versions.

------------------------------------------------------

Without_sentinels_short-prompt: Removed all Sentinels, User and Assistant Prompt was shortened

Note: At first the user prompt had some discrepancies between Userprompt-"response format" and AssistantPrompt "response format": Userprompt - "Response Format (Strict Adherence Required): {left lane | right lane | unclear}; Obstacles are {not on the same lane | far away | near | very close}; Decision: {decision}" | Assistantprompt: "Lane: left lane; Obstacles: far away; Decision: straight forward"

+) First-run_one-epoch_b6_long-lidar: One epoch run time, trained on "trimmed and balanced" trainset, batch 6, Lidar data infused in long-form (all lidar points) as angle-range pairs of numbers at the beginning of userprompt. "Wrong" Userprompt. On Wandb: "12-06-2025_0425_seed42_loss"

+) Second-run_3-epochs_b12_long-lidar: Same as above but batch 12, run for three epochs, to test overfitting when training longer / performance when model see the same samples multiple times. On Wandb:"13-06-2025_0520_seed23_loss"

+) Third-run_one-epoch_b6_short-lidar: One epoch run time, trained on "trimmed and balanced" trainset, batch 6, Lidar data infused in short-form (only three points directly ahead at "179, 180, 181" degree) as angle-range pairs of numbers in the middle of the userprompt. "Wrong" Userprompt. Intended to test if having many tokens for lidar numbers were confusing the model. On Wandb: "14-06-2025_1225_b6-seed23_loss"

+) Third-run_one-epoch_b6_no-lidar: Same as Third-run_one-epoch_b6_short-lidar but with no no lidar info in userprompt. On Wandb:"14-06-2025_1225_b6-seed42_loss_no-lidar"

+) Fourth-run_one-epoch_b6_short-lidar_full-trainset: One epoch run time, trained on full trainset, batch 6, Lidar data infused in short-form (only three points directly ahead at "179, 180, 181" degree) as angle-range pairs of numbers in the middle of the userprompt. Fixed shortened-form of Userprompt. Intended to test if the trimmed and balanced set was causing worsen performance, more imbalanced predictions or if it's due to the shortened userprompt. On Wandb: "15-06-2025_1500_pre-trim_b6-s23_short-lidar_loss"

+) Third-run_one-epoch_b6_short-lidar_rerun-fixed-userprompt: Rerun of Third-run_one-epoch_b6_short-lidar but this time with Userprompt fixed of discrepancies to assistantprompt.

--------------------------------------------
